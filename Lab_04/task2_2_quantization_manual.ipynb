{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e843c43b-417a-4647-a5bc-8261ef3c61a4",
   "metadata": {},
   "source": [
    "# Lab04 Task 2-2: Manual Post-Training Static Quantization\n",
    "\n",
    "In this notebook, we will try to manually quantize the pretrained model.\n",
    "\n",
    "<font color=\"red\">**Only add or modify code between `YOUR CODE START` and `YOUR CODE END`. Don’t change anything outside of these markers.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e004dd1-edb5-4f1c-88e9-49e3b4ad3360",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Please fill in your student id here.\n",
    "student_id = \"313510156\"\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe68ce-bb84-447a-be54-c7a44f4e5b39",
   "metadata": {},
   "source": [
    "### Library Import\n",
    "\n",
    "The libraries you need for this practice are listed below. You can add more if you think they’re necessary. If you’re not sure whether a library is allowed, ask TA in the FB group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b609c78-f0f5-4f25-aa7d-1ec8b3869ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms, models\n",
    "import copy\n",
    "from resnet20_int8 import (\n",
    "    QuantizedTensor,\n",
    "    QuantizedCifarResNet,\n",
    "    QuantizeLayer,\n",
    "    QuantizedConv2d,\n",
    "    QuantizedConvReLU2d,\n",
    "    QuantizedReLU,\n",
    "    QuantizedLinear,\n",
    "    QuantizedAdaptiveAvgPool2d,\n",
    "    QuantizedAdd,\n",
    "    QuantizedFlatten,\n",
    ")\n",
    "import matplotlib\n",
    "\n",
    "##### YOUR CODE START #####\n",
    "\n",
    "# Do you need any additional libraries? If not, you can leave this block empty.\n",
    "# For this task, you must attempt to manually quantize the model. Therefore, using any libraries that perform automatic quantization or calculate scale/zero-point values is prohibited.\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697f14f6-0076-47f8-b819-2b2b32b4f82a",
   "metadata": {},
   "source": [
    "### Device\n",
    "\n",
    "If you have GPU available, you should see \"cuda\" in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a60e3d-c375-4e87-b2b0-ad3920295f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: %s\" % device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9520b-201d-4e40-bf6d-f539dbe2f5d6",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this lab, we will use CIFAR-10 dataset. CIFAR-10 is a widely used image classification dataset consisting of 60,000 color images at 32×32 resolution. It has 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck), with 50,000 training images and 10,000 test images. Due to its small size and balanced categories, CIFAR-10 is commonly used for benchmarking machine learning and computer vision models.\n",
    "\n",
    "CIFAR-10 has both a training set and a test set. Post-training static quantization requires a small subset of the training set for calibration. On the other hand, manually quantizing the convolutional layer weights is a data-free process. The test set is only used at the end to evaluate the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "219993dc-4289-4509-970a-31e89686f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training & test set\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), \n",
    "                         (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False)\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3344f370-2f66-42f3-a8ed-d0383e4982fb",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "\n",
    "In this lab, you do not need to train a model from scratch. We will use a pretrained ResNet20 model instead. ResNet20 is a popular deep learning model for image classification. Its key feature is the use of skip (residual) connections, which make training deep networks easier and more stable.\n",
    "The code below loads the pre-trained model and evaluates its accuracy on the test set, which should be <font color=\"red\">**92.60%**</font>. Please use this model for the subsequent tasks. <font color=\"red\">**Designing and training your own model is not allowed.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a314efc-a14d-496c-b16d-a049b900783e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/bschen/.cache/torch/hub/chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CifarResNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = torch.hub.load('chenyaofo/pytorch-cifar-models', \n",
    "                       'cifar10_resnet20', pretrained=True).to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "857d22f9-4dfe-4c95-9555-070e4f3ba034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 test set: 92.59%\n"
     ]
    }
   ],
   "source": [
    "def test_acc(model_test):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            labels = labels.to(device, non_blocking=True)\n",
    "            outputs = model_test(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print(f'Accuracy on CIFAR-10 test set: {acc:.2f}%')\n",
    "    return acc\n",
    "    \n",
    "_ = test_acc(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d340f3-2552-4cb9-9c9e-20a86d25493e",
   "metadata": {},
   "source": [
    "### Manually Quantizing the Model\n",
    "\n",
    "While PyTorch's FX graph mode is very convenient, it abstracts away the details of how quantized weights are actually calculated. Therefore, in this section, you will attempt to manually quantize the model.\n",
    "\n",
    "To pass this lab, the test accuracy of your manually quantized model must be higher than <font color=\"red\">**90.00%.**</font>\n",
    "\n",
    "<font color=\"red\">**Please be aware of the following rules. Violating them will result in a score of zero for this section:**</font>\n",
    "\n",
    "1. Your modifications to the model are strictly limited to populating the parameters of the `QuantizedCifarResNet` model. Any other operations, including but not limited to retraining, or changing the model architecture, are forbidden.\n",
    "\n",
    "2. You must explicitly show your calculation process. The use of any functions that automatically compute scale / zero_point or gather statistics is prohibited. (The pre-defined observer in the previous task is prohibited, but it is allowed to use `torch.max` and `torch.min`, or define an observer on your own.) Also, you must not directly assign numerical values without demonstrating how they were derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33203a-3244-4f15-a735-854fced5f833",
   "metadata": {},
   "source": [
    "### Introduction to QuantizedCifarResNet\n",
    "\n",
    "`QuantizedCifarResNet` is a modified version of the standard `CifarResNet` architecture, specifically adapted for **integer-only inference**. Unlike the original `CifarResNet` which performs computations using 32-bit floating-point (FP32) numbers, this quantized version primarily uses 8-bit integer arithmetic (`int8` for weights, `uint8` for activations) for most of its operations. This significantly reduces model size and can lead to faster inference speeds on hardware with specialized integer instruction support.\n",
    "\n",
    "The key differences arise from replacing standard PyTorch layers (`nn.Conv2d`, `nn.ReLU`, `nn.Linear`, etc.) with custom-defined quantized layer equivalents. These custom layers require specific **quantization parameters** (scale and zero-point) to map the integer values back to the approximate floating-point range, ensuring the model maintains reasonable accuracy. Data between these layers is passed using a `QuantizedTensor` wrapper object, which bundles the `uint8` tensor data with its corresponding `scale` and `zero_point`.\n",
    "\n",
    "Here's a breakdown of the custom quantized layers used in this implementation and the parameters they typically require **after initialization** (usually set via methods like `set_..._params` or `set_..._weight` after calibration):\n",
    "\n",
    "1.  **`QuantizeLayer`**:\n",
    "    * **Role**: The entry point, converts the initial `float32` input tensor into a `QuantizedTensor` (`uint8`).\n",
    "    * **Parameters Needed**:\n",
    "        * `output_scale (float)`: The scale factor for the output activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the output activation.\n",
    "\n",
    "2.  **`QuantizedConv2d`** (Used for `conv2` in BasicBlock and `downsample`):\n",
    "    * **Role**: Performs 2D convolution using `int8` weights and `uint8` activations, producing a `uint8` output. Uses `fp32` bias.\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`: The quantized weights (typically obtained after fusing BatchNorm).\n",
    "        * `weight_scale (torch.Tensor[float32])`: The **per-channel** scale factor for the weights.\n",
    "        * `weight_zero_point (torch.Tensor[int32])`: The **per-channel** zero-point for the weights.\n",
    "        * `bias_fp32 (torch.Tensor[float32])`: The fused `float32` bias term.\n",
    "        * `output_scale (float)`: The scale factor for the output activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the output activation.\n",
    "\n",
    "3.  **`QuantizedConvReLU2d`** (Used for `conv1` in BasicBlock and the first `conv1` of the network):\n",
    "    * **Role**: Fuses `Conv2d` and `ReLU` operations. Similar to `QuantizedConv2d` but applies ReLU before the final requantization step. Uses `fp32` bias. **The output zero-point is implicitly 0 due to ReLU.**\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`\n",
    "        * `weight_scale (torch.Tensor[float32])` (**Per-channel**)\n",
    "        * `weight_zero_point (torch.Tensor[int32])` (**Per-channel**)\n",
    "        * `bias_fp32 (torch.Tensor[float32])`\n",
    "        * `output_scale (float)` (Output zero-point is fixed to 0 internally).\n",
    "\n",
    "4.  **`QuantizedReLU`**:\n",
    "    * **Role**: Applies ReLU activation directly on the `uint8` tensor by clamping values below the input `zero_point`.\n",
    "    * **Parameters Needed**: None (It's stateless and uses the parameters from the input `QuantizedTensor`).\n",
    "\n",
    "5.  **`QuantizedAdd`**:\n",
    "    * **Role**: Performs element-wise addition of two `QuantizedTensor` inputs (requiring dequantization, float addition, and requantization). Used for residual connections.\n",
    "    * **Parameters Needed**:\n",
    "        * `output_scale (float)`: The scale factor for the resulting summed activation.\n",
    "        * `output_zero_point (int)`: The zero-point for the resulting summed activation.\n",
    "\n",
    "6.  **`QuantizedAdaptiveAvgPool2d`**:\n",
    "    * **Role**: Performs adaptive average pooling on the `uint8` tensor.\n",
    "    * **Parameters Needed**: None (Stateless, passes through the input scale/zero-point after integer averaging).\n",
    "\n",
    "7.  **`QuantizedFlatten`**:\n",
    "    * **Role**: Flattens the `uint8` tensor while preserving scale/zero-point.\n",
    "    * **Parameters Needed**: None (Stateless).\n",
    "\n",
    "8.  **`QuantizedLinear`** (Used as the final `fc` layer):\n",
    "    * **Role**: Performs a linear transformation using `int8` weights and `uint8` input, producing a `float32` output (common for the final classification layer). Uses `fp32` bias.\n",
    "    * **Parameters Needed**:\n",
    "        * `weight_int8 (torch.Tensor[int8])`\n",
    "        * `weight_scale (torch.Tensor[float32])` (**Per-channel/output feature**)\n",
    "        * `weight_zero_point (torch.Tensor[int32])` (**Per-channel/output feature**)\n",
    "        * `bias_fp32 (torch.Tensor[float32])`\n",
    "\n",
    "You can find more details of `QuantizedCifarResNet` in `resnet20_int8.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59e186bf-9d88-47ad-a1b2-2859e94f145a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QuantizedCifarResNet(\n",
       "  (quant): QuantizeLayer(output_scale=1.000000, output_zero_point=0)\n",
       "  (conv1): QuantizedConvReLU2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(16, 32, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0), groups=1, bias=True)\n",
       "      )\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (downsample): Sequential(\n",
       "        (0): QuantizedConv2d(32, 64, kernel_size=(1, 1), stride=(2, 2), padding=(0, 0), groups=1, bias=True)\n",
       "      )\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): QuantizedConvReLU2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (conv2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1, bias=True)\n",
       "      (relu): QuantizedReLU(Quantized ReLU (uint8 clamp at zero_point))\n",
       "      (add): QuantizedAdd()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): QuantizedAdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (flat): QuantizedFlatten()\n",
       "  (fc): QuantizedLinear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manual = QuantizedCifarResNet().to(device)\n",
    "model_manual.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b60bf-2a8d-4b00-b2f3-04da838feefb",
   "metadata": {},
   "source": [
    "#### 1. Prepare the Model\n",
    "\n",
    "Let's manually quantize the model step-by-step. We'll start with the preparation phase. Although PyTorch's FX graph mode offers `prepare_fx` to automatically insert observers and fuse layers (e.g., `Conv2d`, `BatchNorm2d`, `ReLU`), we need to do this manually here. So, we'll define our own observer now and insert it into the FP32 model. Layer fusion mainly concerns weight recalculation, so we'll handle that later in step three. Below is an example of defining a min/max observer and inserting it into the initial `bn1` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb47e3af-b2ab-45c8-a399-095709a0c3ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered 32 observers:\n",
      "  - quant\n",
      "  - bn1\n",
      "  - layer1.0.bn1\n",
      "  - layer1.0.add\n",
      "  - layer1.0.bn2\n",
      "  - layer1.1.bn1\n",
      "  - layer1.1.add\n",
      "  - layer1.1.bn2\n",
      "  - layer1.2.bn1\n",
      "  - layer1.2.add\n",
      "  - layer1.2.bn2\n",
      "  - layer2.0.bn1\n",
      "  - layer2.0.add\n",
      "  - layer2.0.bn2\n",
      "  - layer2.0.downsample.1\n",
      "  - layer2.1.bn1\n",
      "  - layer2.1.add\n",
      "  - layer2.1.bn2\n",
      "  - layer2.2.bn1\n",
      "  - layer2.2.add\n",
      "  - layer2.2.bn2\n",
      "  - layer3.0.bn1\n",
      "  - layer3.0.add\n",
      "  - layer3.0.bn2\n",
      "  - layer3.0.downsample.1\n",
      "  - layer3.1.bn1\n",
      "  - layer3.1.add\n",
      "  - layer3.1.bn2\n",
      "  - layer3.2.bn1\n",
      "  - layer3.2.add\n",
      "  - layer3.2.bn2\n",
      "  - fc\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Define the Observer class (can be used as a hook)\n",
    "class Observer:\n",
    "    def __init__(self, obs_skip_connect: bool=False):\n",
    "        # Initialize statistics\n",
    "        self.min_val = float('inf')\n",
    "        self.max_val = float('-inf')\n",
    "\n",
    "        # For skip connection\n",
    "        self.obs_skip_connect = obs_skip_connect\n",
    "        self.obs_skip_connect_counter = 0\n",
    "\n",
    "    def __call__(self, module: nn.Module, inputs: tuple[torch.Tensor], output: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Hook function executed after the module's forward pass.\n",
    "        \"\"\"\n",
    "        if not self.obs_skip_connect:\n",
    "            batch_min = output.detach().min().item()\n",
    "            batch_max = output.detach().max().item()\n",
    "        else:\n",
    "            # For skip connections, observe the input tensors instead.\n",
    "            batch_min = inputs[0].detach().min().item()\n",
    "            batch_max = inputs[0].detach().max().item()\n",
    "\n",
    "        # Update overall min/max seen so far.\n",
    "        if not self.obs_skip_connect or self.obs_skip_connect_counter % 2 == 1:\n",
    "            self.min_val = min(self.min_val, batch_min)\n",
    "            self.max_val = max(self.max_val, batch_max)\n",
    "\n",
    "        # Counter for reuse ReLU module\n",
    "        if self.obs_skip_connect:\n",
    "            self.obs_skip_connect_counter = (self.obs_skip_connect_counter + 1) % 2\n",
    "\n",
    "    def get_min_max(self) -> tuple[float, float]:\n",
    "        # Returns the overall observed min and max values.\n",
    "        return self.min_val, self.max_val\n",
    "\n",
    "    def reset(self):\n",
    "        # Resets the observed min/max values.\n",
    "        self.min_val = float('inf')\n",
    "        self.max_val = float('-inf')\n",
    "\n",
    "\n",
    "# Create deepcopy of model\n",
    "model_prepared = copy.deepcopy(model)\n",
    "\n",
    "# Dictionary to hold observers for later inspection\n",
    "observers, removers = {}, {}\n",
    "\n",
    "# Input quantization layer\n",
    "observers[\"quant\"] = Observer()\n",
    "removers[\"quant\"] = None\n",
    "\n",
    "# Register observers to layers\n",
    "for name, module in model_prepared.named_modules():\n",
    "    if isinstance(module, (nn.Linear, nn.BatchNorm2d)):\n",
    "        obs = Observer()\n",
    "        observers[name] = obs\n",
    "        removers[name] = module.register_forward_hook(obs)\n",
    "    elif isinstance(module, nn.ReLU) and \"layer\" in name:\n",
    "        # For ReLU, observe the input to capture pre-ReLU activations\n",
    "        obs = Observer(obs_skip_connect=True)\n",
    "        observers[name.replace(\"relu\", \"add\")] = obs\n",
    "        removers[name.replace(\"relu\", \"add\")] = module.register_forward_hook(obs)\n",
    "\n",
    "print(f\"Registered {len(observers)} observers:\")\n",
    "for k in observers.keys():\n",
    "    print(f\"  - {k}\")\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26372dd9-4373-47e8-aa24-839f3fa52e74",
   "metadata": {},
   "source": [
    "#### 2. Calibrate the Model\n",
    "\n",
    "Next, we need to calibrate the model. This step is quite similar to the process when using PyTorch's FX graph mode. It simply involves feeding data from the training set (or a representative subset of it) through the model we prepared earlier (the one with observers attached). Please note: <font color=\"red\">**it is crucial not to use the test set data for calibration.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854bd203-9a73-4cf8-9c6e-90425c9c5923",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "# Prepare training data for calibration\n",
    "train_size = 10000\n",
    "train_subset = torch.utils.data.Subset(trainset, range(train_size))\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=128, shuffle=False)\n",
    "\n",
    "# Calibrate with training data\n",
    "model_prepared.eval()\n",
    "with torch.no_grad():\n",
    "    for images, _ in trainloader:\n",
    "        images = images.to(device)\n",
    "        observers[\"quant\"](None, None, images)\n",
    "        model_prepared(images)\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f59cc5-0173-48a0-a9c8-7e07b02cc6ac",
   "metadata": {},
   "source": [
    "#### 3. Convert the Model\n",
    "\n",
    "Finally, we need to populate the `QuantizedCifarResNet` with its parameters. You will need to iterate through all layers in the quantized model and set the required parameters (such as quantized weights, scales, zero-points, and biases) based on their type. The necessary data should be obtained from the original model and the observers inserted previously. Additionally, note that consecutive `Conv2d`, `BatchNorm2d`, and `ReLU` layers in the original model have been fused into corresponding single layers in `QuantizedCifarResNet`. You must adjust the `Conv2d` weights and biases according to the parameters of the corresponding `BatchNorm2d` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e3e1a26-3573-4201-b85d-ed43a257d4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantizing module  1/58: \n",
      "Quantizing module  2/58: quant\n",
      "Quantizing module  3/58: conv1\n",
      "Quantizing module  4/58: layer1\n",
      "Quantizing module  5/58: layer1.0\n",
      "Quantizing module  6/58: layer1.0.conv1\n",
      "Quantizing module  7/58: layer1.0.conv2\n",
      "Quantizing module  8/58: layer1.0.relu\n",
      "Quantizing module  9/58: layer1.0.add\n",
      "Quantizing module 10/58: layer1.1\n",
      "Quantizing module 11/58: layer1.1.conv1\n",
      "Quantizing module 12/58: layer1.1.conv2\n",
      "Quantizing module 13/58: layer1.1.relu\n",
      "Quantizing module 14/58: layer1.1.add\n",
      "Quantizing module 15/58: layer1.2\n",
      "Quantizing module 16/58: layer1.2.conv1\n",
      "Quantizing module 17/58: layer1.2.conv2\n",
      "Quantizing module 18/58: layer1.2.relu\n",
      "Quantizing module 19/58: layer1.2.add\n",
      "Quantizing module 20/58: layer2\n",
      "Quantizing module 21/58: layer2.0\n",
      "Quantizing module 22/58: layer2.0.conv1\n",
      "Quantizing module 23/58: layer2.0.conv2\n",
      "Quantizing module 24/58: layer2.0.relu\n",
      "Quantizing module 25/58: layer2.0.downsample\n",
      "Quantizing module 26/58: layer2.0.downsample.0\n",
      "Quantizing module 27/58: layer2.0.add\n",
      "Quantizing module 28/58: layer2.1\n",
      "Quantizing module 29/58: layer2.1.conv1\n",
      "Quantizing module 30/58: layer2.1.conv2\n",
      "Quantizing module 31/58: layer2.1.relu\n",
      "Quantizing module 32/58: layer2.1.add\n",
      "Quantizing module 33/58: layer2.2\n",
      "Quantizing module 34/58: layer2.2.conv1\n",
      "Quantizing module 35/58: layer2.2.conv2\n",
      "Quantizing module 36/58: layer2.2.relu\n",
      "Quantizing module 37/58: layer2.2.add\n",
      "Quantizing module 38/58: layer3\n",
      "Quantizing module 39/58: layer3.0\n",
      "Quantizing module 40/58: layer3.0.conv1\n",
      "Quantizing module 41/58: layer3.0.conv2\n",
      "Quantizing module 42/58: layer3.0.relu\n",
      "Quantizing module 43/58: layer3.0.downsample\n",
      "Quantizing module 44/58: layer3.0.downsample.0\n",
      "Quantizing module 45/58: layer3.0.add\n",
      "Quantizing module 46/58: layer3.1\n",
      "Quantizing module 47/58: layer3.1.conv1\n",
      "Quantizing module 48/58: layer3.1.conv2\n",
      "Quantizing module 49/58: layer3.1.relu\n",
      "Quantizing module 50/58: layer3.1.add\n",
      "Quantizing module 51/58: layer3.2\n",
      "Quantizing module 52/58: layer3.2.conv1\n",
      "Quantizing module 53/58: layer3.2.conv2\n",
      "Quantizing module 54/58: layer3.2.relu\n",
      "Quantizing module 55/58: layer3.2.add\n",
      "Quantizing module 56/58: avgpool\n",
      "Quantizing module 57/58: flat\n",
      "Quantizing module 58/58: fc\n"
     ]
    }
   ],
   "source": [
    "##### YOUR CODE START #####\n",
    "\n",
    "def fuse_conv_bn(\n",
    "    conv: nn.Conv2d,\n",
    "    bn: nn.BatchNorm2d\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Fuse convolution and batch normalization parameters.\"\"\"\n",
    "    w = conv.weight.detach().cpu()\n",
    "    if conv.bias is None:\n",
    "        b = torch.zeros(w.size(0))\n",
    "    else:\n",
    "        b = conv.bias.detach().cpu()\n",
    "    gamma = bn.weight.detach().cpu()\n",
    "    beta = bn.bias.detach().cpu()\n",
    "    mu = bn.running_mean.detach().cpu()\n",
    "    var = bn.running_var.detach().cpu()\n",
    "    eps = bn.eps\n",
    "\n",
    "    std = torch.sqrt(var + eps)\n",
    "    scale = (gamma / std).reshape(-1, 1, 1, 1)\n",
    "    w_fused = w * scale\n",
    "    b_fused = beta + (gamma / std) * (b - mu)\n",
    "    return w_fused, b_fused\n",
    "\n",
    "def calc_quant_params(\n",
    "    min_val: float,\n",
    "    max_val: float,\n",
    "    num_bits: int=8,\n",
    "    symmetric: bool=False,\n",
    "    unsigned: bool=False\n",
    "    ) -> tuple[float, int]:\n",
    "    \"\"\"Calculate scale and zero-point for quantization.\"\"\"\n",
    "    if symmetric:\n",
    "        max_abs = max(abs(min_val), abs(max_val))\n",
    "        scale = max_abs / (2 ** (num_bits - 1) - 1)\n",
    "        zero_point = 0\n",
    "    else:\n",
    "        if unsigned:\n",
    "            qmin, qmax = 0, 2**num_bits - 1\n",
    "        else:\n",
    "            qmin, qmax = -1 * 2**(num_bits - 1), 2**(num_bits - 1) - 1\n",
    "        scale = (max_val - min_val) / float(qmax - qmin) * 0.99\n",
    "        zero_point = int(round(qmin - min_val / scale))\n",
    "        zero_point = torch.clamp(torch.tensor(zero_point), qmin, qmax).item()\n",
    "    return scale, zero_point\n",
    "\n",
    "# Fill in quantized model parameters\n",
    "for i, (name, module) in enumerate(model_manual.named_modules()):\n",
    "    print(f\"Quantizing module {i+1:2d}/{len(list(model_manual.named_modules()))}: {name}\")\n",
    "\n",
    "    # QuantizeLayer (input/output quantization node)\n",
    "    if isinstance(module, QuantizeLayer):\n",
    "        min_val, max_val = observers[\"quant\"].get_min_max()\n",
    "        out_scale, out_zero_point = calc_quant_params(min_val, max_val, num_bits=8, unsigned=True)\n",
    "        module.set_output_quant_params(out_scale, out_zero_point)\n",
    "\n",
    "    # QuantizedConv2d / QuantizedConvReLU2d\n",
    "    elif isinstance(module, (QuantizedConv2d, QuantizedConvReLU2d)):\n",
    "        bname = name.replace(\"conv\", \"bn\") if \"conv\" in name else name[:-1] + \"1\"\n",
    "\n",
    "        # Weight quantization (per-output-channel)\n",
    "        with torch.no_grad():\n",
    "            # Fuse conv and bn parameters\n",
    "            w, bias_fp32 = fuse_conv_bn(\n",
    "                dict(model.named_modules())[name],  # Convolution layer\n",
    "                dict(model.named_modules())[bname]  # BatchNorm layer\n",
    "            )\n",
    "\n",
    "            # (channel_out, channel_in, kernel_h, kernel_w) -> (channel_out, -1) -> (channel_out,)\n",
    "            w_min = torch.min(w.view(w.size(0), -1), dim=1).values\n",
    "            w_max = torch.max(w.view(w.size(0), -1), dim=1).values\n",
    "\n",
    "            # Quantization\n",
    "            weight_scale = torch.zeros((w.size(0),), dtype=torch.float32)\n",
    "            weight_zero_point = torch.zeros((w.size(0),), dtype=torch.int32)\n",
    "            weight_int8 = torch.zeros_like(w, dtype=torch.int8)\n",
    "            for i in range(w.size(0)):\n",
    "                weight_scale[i], weight_zero_point[i] = calc_quant_params(\n",
    "                    w_min[i].item(), w_max[i].item()\n",
    "                )\n",
    "                weight_int8[i] = torch.clamp((\n",
    "                    w[i] / weight_scale[i] + weight_zero_point[i]\n",
    "                ).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "        module.set_weight_quant_params(weight_scale, weight_zero_point)\n",
    "        module.set_int8_weight(weight_int8)\n",
    "        module.set_fp32_bias(bias_fp32)\n",
    "\n",
    "        # Output quantization\n",
    "        min_val, max_val = observers[bname].get_min_max()\n",
    "        if isinstance(module, QuantizedConvReLU2d):\n",
    "            min_val = max(0.0, min_val)  # ReLU clamps negative values to zero\n",
    "        out_scale, out_zero_point = calc_quant_params(min_val, max_val, num_bits=8, unsigned=True)\n",
    "\n",
    "        if isinstance(module, QuantizedConvReLU2d):\n",
    "            module.set_output_quant_params(out_scale)\n",
    "        else:\n",
    "            module.set_output_quant_params(out_scale, out_zero_point)\n",
    "\n",
    "    # QuantizedAdd (for skip connections)\n",
    "    elif isinstance(module, QuantizedAdd):\n",
    "        min_val, max_val = observers[name].get_min_max()\n",
    "        output_scale, output_zero_point = calc_quant_params(min_val, max_val, num_bits=8, unsigned=True)\n",
    "        module.set_output_quant_params(output_scale, output_zero_point)\n",
    "\n",
    "    # QuantizedLinear\n",
    "    elif isinstance(module, QuantizedLinear):\n",
    "        # Weight quantization (symmetric, per-tensor)\n",
    "        with torch.no_grad():\n",
    "            fc = dict(model.named_modules())[name]\n",
    "            w = fc.weight.detach().cpu()\n",
    "            bias_fp32 = fc.bias.detach().cpu()\n",
    "\n",
    "            # (channel_out, channel_in) -> (channel_out,)\n",
    "            w_min = torch.min(w, dim=1).values\n",
    "            w_max = torch.max(w, dim=1).values\n",
    "\n",
    "            # Quantization\n",
    "            weight_scale = torch.zeros((w.size(0),), dtype=torch.float32)\n",
    "            weight_zero_point = torch.zeros((w.size(0),), dtype=torch.int32)\n",
    "            weight_int8 = torch.zeros_like(w, dtype=torch.int8)\n",
    "            for i in range(w.size(0)):\n",
    "                weight_scale[i], weight_zero_point[i] = calc_quant_params(\n",
    "                    w_min[i].item(), w_max[i].item()\n",
    "                )\n",
    "                weight_int8[i] = torch.clamp((\n",
    "                    w[i] / weight_scale[i] + weight_zero_point[i]\n",
    "                ).round(), -128, 127).to(torch.int8)\n",
    "\n",
    "        module.set_weight_quant_params(weight_scale, weight_zero_point)\n",
    "        module.set_int8_weight(weight_int8)\n",
    "        module.set_fp32_bias(bias_fp32)\n",
    "\n",
    "##### YOUR CODE END #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6de2e186-d070-48c3-ad01-2cf82a3c37bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on CIFAR-10 test set: 92.70%\n",
      "\n",
      "===========================================\n",
      "\n",
      "Congratulations! You've achieved the goal of this task. Remember to save your model!\n",
      "You can also try increasing accuracy further to earn a higher score!\n"
     ]
    }
   ],
   "source": [
    "# Let's see the result.\n",
    "acc = test_acc(model_manual)\n",
    "\n",
    "print(\"\\n===========================================\\n\")\n",
    "\n",
    "if acc < 90.0:\n",
    "    print(\"Oh no! Your test accuracy is too low!\")\n",
    "else:\n",
    "    print(\"Congratulations! You've achieved the goal of this task. Remember to save your model!\")\n",
    "    print(\"You can also try increasing accuracy further to earn a higher score!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46829a08-661e-40fc-a291-e367b4497797",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "You can use the code below to save your model as `[student_id]_quantization.pt`, where `[student_id]` is replaced by your student ID in the first cell of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15e79133-d139-40fa-a3a1-12ec6660a304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model is saved to \"313510156_quantization.pt\".\n"
     ]
    }
   ],
   "source": [
    "file_name = student_id + \"_quantization.pt\"\n",
    "# Save model.state_dict() instead of the entire model.\n",
    "torch.save(model_manual.state_dict(), file_name)\n",
    "print(\"Your model is saved to \\\"\" + file_name + \"\\\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6845b-7899-4fe2-b330-15c2d36aff93",
   "metadata": {},
   "source": [
    "### Final Check\n",
    "\n",
    "TA has provided check_quantization.py for students to check if their models can pass the tests. <font color=\"red\">**Please make sure to check it before submission.**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d9f03e7-f966-403f-87c4-f83ecfac90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! You've achieved the goals of this task.\n",
      "Your model's test accuracy is 92.70%.\n"
     ]
    }
   ],
   "source": [
    "!python check_quantization.py --path {file_name}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
